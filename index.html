<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Predict Ego-Exocentric Conversational Graph from egocentric audio-visual input.">
  <meta name="keywords" content="avconv, egoexoconv, convgraph">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Audio-Visual Conversational Graph</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" type="image/png" href="assets/gt-logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero bg-section">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="title-wrapper">

          <h1 class="title is-1 publication-title">
            The Audio-Visual Conversational Graph: <br/>
            From an Ego-Exocentric Perspective</h1>



          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://vjwq.github.io/">Wenqi Jia</a><sup>1,2*</sup>,&nbsp;
              <a href="https://aptx4869lm.github.io/">Miao Liu</a><sup>4</sup>,&nbsp;
              <a href="https://www.hao-jiang.net/">Hao Jiang</a>,<sup>2</sup>,&nbsp;
              <a href="https://www.ishwarya.me/">Ishwarya Ananthabhotla</a><sup>2</sup>,<br>
              <a href="https://rehg.org/">James M. Rehg</a><sup>3</sup>,&nbsp;
              <a href="https://www.vamsiithapu.com/">Vamsi Krishna Ithapu</a><sup>2</sup>,&nbsp;
              <a href="https://ruohangao.github.io/">Ruohan Gao</a><sup>2</sup>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Georgia Institute of Technology,</span>
            <span class="author-block"><sup>2</sup>Meta Reality Labs Research,</span>
            <span class="author-block"><sup>3</sup>UIUC,</span>
            <span class="author-block"><sup>4</sup>GenAI, Meta</span>
            <div style="width:100%;text-align:right"><small><i>* This work was done during Wenqi's internship at Reality Labs</i></small></div>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jia_The_Audio-Visual_Conversational_Graph_From_an_Egocentric-Exocentric_Perspective_CVPR_2024_paper.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Jia_The_Audio-Visual_Conversational_CVPR_2024_supplemental.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.12870"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Poster. -->
              <span class="link-block">
                <a href="assets/AVConv-poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/VJWQ/AV-CONV.git"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span><font color="#ABB0B8">Data</font></span>
                  </a>
            </div>
          </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">

  <div class="container is-max-desktop">
    <div style="text-align: center;">
        <img src="assets/teaser1.png"  style="width:70%;max-width:70%">
        <img src="assets/teaser2.png"  style="width:70%;max-width:70%">

      <h2 class="subtitle has-text-centered">

        Our Ego-Exocentric Conversational Graph Prediction task jointly learns <br>
        the egocentric conversational behaviors, and the exocentric conversational behaviors.

      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The recent thriving development of research related to egocentric videos has provided a 
            unique perspective for the study of conversational interactions, where both visual and 
            audio signals play a crucial role. 
          </p>
          <p>
            We introduce the Ego-Exocentric Conversational Graph Prediction problem, 
            marking the first attempt to infer exocentric conversational interactions 
            from egocentric videos. 
            We propose a unified multi-modal framework---Audio-Visual Conversational Attention (AV-CONV), 
            for the joint prediction of conversation behaviors---speaking and listening---for 
            both the camera wearer as well as all other social partners present in the egocentric video. 
            Specifically, we adopt the self-attention mechanism to model the representations across-time, 
            across-subjects, and across-modalities. To validate our method, we conduct experiments on a 
            challenging egocentric video dataset that includes multi-speaker and multi-conversation scenarios. 
          </p>
          <p>
            Our results demonstrate the superior performance of our method compared to a series of baselines. 
            We also present detailed ablation studies to assess the contribution of each component in our model. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <img src="assets/model.png"  style="width:100%;max-width:100%">
          <!-- Our model takes multiple egocentric frames and multi-channel audio signals.  -->
          <p>
            <b>(a)</b> For each frame, faces of social partners are cropped to serve as raw visual input, 
            while their corresponding head positions are concatenated with audio inputs to generate positional 
            audio signals. Visual and audio signals are encoded by two separate ResNet18 Backbones and are 
            concatenated to produce Audio-Visual features for each cropped head. 
          </p>
          <p>
            <b>(b)</b> After obtaining temporal Audio-Visual feature tubes of video length, they are flattened 
            into a token to be fed into the Conversational Attention Module to produce augmented Single Head 
            Feature feature. Egocentric Classifiers directly take them to predict Egocentric Edge Attributes, 
            and pairs of these features are arbitrarily combined to generate pairwise audio-visual features to 
            predict Exocentric Edge Attributes.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->


    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demo Video</h2>
        

        
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <section id="Demo Video" style="padding-top: 80px; margin-top: -80px;">
          <!-- <h2 class="ui center aligned header" style="font-size: 1.5em; font-weight: normal; margin-bottom: 0.3em; margin-top: 0.8em;">Demo Video</h2> -->
          <!-- <video muted autoplay loop style="width:25%;vertical-align:middle;"> -->
            <p style="text-align:left;">
              <b>Left</b>: Original input sample with audio <br>
              <b>Right</b>: 5x slow motion input with predictions and ground truth label
            </p>
        <div>
            <video controls="controls" style="width:24%;vertical-align:middle;">
                <source src="assets/demos/regular_speed_video/1035.mp4" type="video/mp4">
            </video>
            <video controls="controls" style="width:74%;vertical-align:middle;">
                <source src="assets/demos/slowmotion/1035_slow.mp4" type="video/mp4">
            </video>
        </div>
        <p style="text-align:left;">
          Three speakers engaged in concurrent conversations, exhibiting pause-and-resume patterns. 
        </p>

        <div>
            <video controls="controls" style="width:24%;vertical-align:middle;">
                <source src="assets/demos/regular_speed_video/933.mp4" type="video/mp4">
            </video>
            <video controls="controls" style="width:74%;vertical-align:middle;">
            <source src="assets/demos/slowmotion/933_slow.mp4" type="video/mp4">
            </video>
        </div>
        <p style="text-align:left;">
          The yellow-boxed participant joins the conversation toward the end of the sequence.
        </p>

        <div>
            <video controls="controls" style="width:24%;vertical-align:middle;">
                <source src="assets/demos/regular_speed_video/825.mp4" type="video/mp4">
            </video>
            <video controls="controls" style="width:74%;vertical-align:middle;">
            <source src="assets/demos/slowmotion/825_slow.mp4" type="video/mp4">
            </video>
        </div>
        <p style="text-align:left;">
          The observer (ego agent) stops speaking in the latter part of the sequence.
        </p>

        <video controls="controls" style="width:24%;vertical-align:middle;">
            <source src="assets/demos/regular_speed_video/1032.mp4" type="video/mp4">
        </video>
        <video controls="controls" style="width:74%;vertical-align:middle;">
          <source src="assets/demos/slowmotion/1032_slow.mp4" type="video/mp4">
        </video>
        <p style="text-align:left;">
          The pink-boxed participant is not visible due to the limited egocentric field of view.
        </p>

    </td>
  </tr>
</tbody></table>

</div>
</div>
<!--/ Paper video. -->
</div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{jia2024audio,
        &nbsp; title={The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective},
        &nbsp; author={Jia, Wenqi and Liu, Miao and Jiang, Hao and Ananthabhotla, Ishwarya and Rehg, James M and Ithapu, Vamsi Krishna and Gao, Ruohan},
        &nbsp; booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
        &nbsp; year={2024}
      }
    </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  </div>
</section>

</body>
</html>
